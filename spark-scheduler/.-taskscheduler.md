 Task运行期之函数调用关系分析&存储子系统分析

摘要：本篇主要讲述在TaskRunner中执行的业务逻辑是如何被调用到的，另外试图运行着的task任务的输入数据从哪获取，处理结果返回到哪里，如何返回；以及对存储子系统进行分析。

一、Task运行期之函数调用关系分析

RDD的转换过程
还是以最简单的wordcount为例说明rdd的转换过程
sc.textFile("README.md").flatMap(line => line.split(" ")).map(word => (word, 1))

RDD转换小结
小结一下整个RDD转换过程
HadoopRDD->MappedRDD->FlatMappedRDD->MappedRDD->PairRDDFunctions->ShuffleRDD->MapPartitionsRDD
整个转换过程好长啊，这一切的转换都发生在任务提交之前。

## 运行过程分析
### 数据集操作分类
在对任务运行过程中的函数调用关系进行分析之前，我们也来探讨一个偏理论的东西，作用于RDD之上的Transformation是什么样子的
对这个问题的解答和数学搭上关系了，从理论抽象的角度来说，任务处理可归结为“input -> processing -> output"。input和output对应于数据集dataset
在此基础上作一下简单的分类：
  1. one-one  一个dataset在转换之后还是一个dataset，而且dataset的size不变，如map
  2. one-one  一个dataset在转换之后还是一个dataset，但size发生更改，这种更改有可能扩大或缩小，如flatMap是size增大操作，而substract是size变小的操作
  3. many-one 多个dataset合并为一个dataset，如combine，join
  4. one-many 一个dataset分裂为多个dataset，如groupBy

---

## TaskScheduler源码与任务提交原理浅析
  
TaskScheduler调度入口：

* TaskSchedulerImpl.submitTasks
  * 任务（tasks）会被包装成TaskSetManager(由于TaskSetManger不是线程安全的，所以源码中需要进行同步) 
  * TaskSetManager实例通过SchedulerBuilder（分为FIFOSchedulerBuilder和FairSchedulerBuilder）投入调度池中等待调度
  * 任务提交同时启动定时器，如果任务还未被执行，定时器会持续发出警告直到任务执行
  * 调用backend的reviveOffers函数，向backend的DriverEndPoint实例发送ReviveOffers消息，DriverEndPoint收到ReviveOffers消息后，调用makeOffers处理函数
  ```
    override def submitTasks(taskSet: TaskSet) {
      val tasks = taskSet.tasks
      logInfo("Adding task set " + taskSet.id + " with " + tasks.length + " tasks")
      this.synchronized {
        val manager = createTaskSetManager(taskSet, maxTaskFailures)
        val stage = taskSet.stageId
        val stageTaskSets =
          taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])
        stageTaskSets(taskSet.stageAttemptId) = manager
        val conflictingTaskSet = stageTaskSets.exists { case (_, ts) =>
          ts.taskSet != taskSet && !ts.isZombie
        }
        if (conflictingTaskSet) {
          throw new IllegalStateException(s"more than one active taskSet for stage $stage:" +
            s" ${stageTaskSets.toSeq.map{_._2.taskSet.id}.mkString(",")}")
        }
        schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)

        if (!isLocal && !hasReceivedTask) {
          starvationTimer.scheduleAtFixedRate(new TimerTask() {
            override def run() {
              if (!hasLaunchedTask) {
                logWarning("Initial job has not accepted any resources; " +
                  "check your cluster UI to ensure that workers are registered " +
                  "and have sufficient resources")
              } else {
                this.cancel()
              }
            }
          }, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS)
        }
        hasReceivedTask = true
      }
      backend.reviveOffers()
    }

  ```

* BackEnd类继承CoarseGrainedScheduler类，CoarseGrainedScheduler在启动时会创建DriverEndPoint。而DriverEndPoint每隔一定时间（spark.scheduler.revive.interval, 默认为1s）进行一次调度（给自身发送reviveOffers消息，调用makeOffers进行调度）。代码如下所示：
  ```
  override def onStart() {
      // Periodically revive offers to allow delay scheduling to work
      val reviveIntervalMs = conf.getTimeAsMs("spark.scheduler.revive.interval", "1s")

      reviveThread.scheduleAtFixedRate(new Runnable {
        override def run(): Unit = Utils.tryLogNonFatalError {
          Option(self).foreach(_.send(ReviveOffers))
        }
      }, 0, reviveIntervalMs, TimeUnit.MILLISECONDS)
    }

  ```

* 当Executor执行分配的任务时，会持续向Driver发送StatusUpdate状态消息，通过调用Backend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)上报状态及完成结果。CoarseGrainedExecutorBackend会向Driver（DriverEndPoint）发送StatusUpdate消息，消息统一由receive函数处理：
  ```
    override def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer) {
      val msg = StatusUpdate(executorId, taskId, state, data)
      driver match {
        case Some(driverRef) => driverRef.send(msg)
        case None => logWarning(s"Drop $msg because has not yet connected to driver")
      }
    }
  ```

* Driver接收到StatusUpdate消息后将会触发调度（makeOffers），为完成任务的Executor分配任务：
  ```
    override def receive: PartialFunction[Any, Unit] = {
          case StatusUpdate(executorId, taskId, state, data) =>
            scheduler.statusUpdate(taskId, state, data.value)
            if (TaskState.isFinished(state)) {
              executorDataMap.get(executorId) match {
                case Some(executorInfo) =>
                  executorInfo.freeCores += scheduler.CPUS_PER_TASK
                  makeOffers(executorId)
                case None =>
                  // Ignoring the update since we don't know about the executor.
                  logWarning(s"Ignored task status update ($taskId state $state) " +
                    s"from unknown executor with ID $executorId")
              }
            }

          case ReviveOffers =>
            makeOffers()

          case KillTask(taskId, executorId, interruptThread) =>
            executorDataMap.get(executorId) match {
              case Some(executorInfo) =>
                executorInfo.executorEndpoint.send(KillTask(taskId, executorId, interruptThread))
              case None =>
                // Ignoring the task kill since the executor is not registered.
                logWarning(s"Attempted to kill task $taskId for unknown executor $executorId.")
            }

        }

  ```
---

#### DriverEndPoint部分函数实现

* reviveOffers()
  ```
  override def reviveOffers() {
    driverEndpoint.send(ReviveOffers)
  }
  ```
发送ReviveOffers消息，DriverEndPoint的receive函数收到ReviveOffers消息后，调用makeOffers函数处理


* makeOffers()
  * makeOffers函数的处理逻辑是：
    * 找到空闲的Executor，分发策略是随机分发的，即尽可能平摊到各个Exector
    * 如果有空闲的Executor，就将任务列表中的部分利用launchTasks发送给指定的Executor
  ```
    // Make fake resource offers on all executors
    private def makeOffers() {
      // Filter out executors under killing
      val activeExecutors = executorDataMap.filterKeys(!executorsPendingToRemove.contains(_))
      val workOffers = activeExecutors.map { case (id, executorData) =>
        new WorkerOffer(id, executorData.executorHost, executorData.freeCores)
      }.toSeq
      launchTasks(scheduler.resourceOffers(workOffers))
    }

  ```
  SchedulerBackend（实际上是CoraseGrainSchedulerBackend）负责将创建的Task分发给Executor

* resourceOffers()
  任务是随机分发给各个Executor的，资源分配的工作由resourceOffers函数处理。
  正如上面submitTasks函数提到的，在TaskSchedulerImpl中，这一组Task被交给一个新的TaskSetManager实例进行管理，所有的TaskSetManager都要经过SchedulerBuilder根据特定的调度策略进行排序。
  在TaskSchedulerImpl的resourceOffers函数中，当前被选择的TaskSetManager的ResourceOffer函数被调用并返回包含了序列化任务数据的TaskDescription，最后这些TaskDescription再由SchedulerBackend派发到ExecutorBackend去执行。

  resourceOffers主要做了3件事：
    * 从Wprkers里随机抽出一些任务执行
    * 通过TaskSetManager找出和Worker在一起的Task，最后编译打包成TaskDescription返回。
    * 将Worker -> Array[TaskDescription]的映射关系返回
    代码解释见 TaskScheduler-Source.md
 
  ```
   /**
   * Called by cluster manager to offer resources on slaves. We respond by asking our active task
   * sets for tasks in order of priority. We fill each node with tasks in a round-robin manner so
   * that tasks are balanced across the cluster.
   */
  def resourceOffers(offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]] = synchronized {
    // Mark each slave as alive and remember its hostname    
    // Also track if new executor is added
    // 标记每个活着的slave节点，并且记住它的hostname，以及记录新添加的executor
    var newExecAvail = false
    // 遍历worker提供的资源，更新executor相关的映射
    for (o <- offers) {
      executorIdToHost(o.executorId) = o.host
      activeExecutorIds += o.executorId
      if (!executorsByHost.contains(o.host)) {
        executorsByHost(o.host) = new HashSet[String]()
        executorAdded(o.executorId, o.host)
        newExecAvail = true
      }
      for (rack <- getRackForHost(o.host)) {
        hostsByRack.getOrElseUpdate(rack, new HashSet[String]()) += o.host
      }
    }

    // Randomly shuffle offers to avoid always placing tasks on the same set of workers.
    // 随机选出一些offers，防止任务都放置在同一个集群
    val shuffledOffers = Random.shuffle(offers)
    // Build a list of tasks to assign to each worker.
    // 建立一个列表的任务，分配给每个worker
    val tasks = shuffledOffers.map(o => new ArrayBuffer[TaskDescription](o.cores))
    val availableCpus = shuffledOffers.map(o => o.cores).toArray
    // getSortedTaskSetQueue函数对TaskSet进行排序
    val sortedTaskSets = rootPool.getSortedTaskSetQueue
    for (taskSet <- sortedTaskSets) {
      logDebug("parentName: %s, name: %s, runningTasks: %s".format(
        taskSet.parent.name, taskSet.name, taskSet.runningTasks))
      if (newExecAvail) {
        taskSet.executorAdded()
      }
    }

    // Take each TaskSet in our scheduling order, and then offer it each node in increasing order
    // of locality levels so that it gets a chance to launch local  on all of them.
    // NOTE: the preferredLocality order: PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY
    // 获取每个TaskSet在我们调度中的顺序，然后把它提供给按照点按照本地化水平排序后的节点，这样它们就都有机会在本地运行
    // 本地性是根据当前的等待时间来确定的任务本地性的级别
    // NOTE: 优先本地性的顺序：PROCESS_LOCAL, NODE_LOCAL, RACK_LOCAL, ANY.

    // 1. 首先依次遍历sortedTaskSets, 并对于每个taskSet，遍历TaskLocality
    // 2. 越local越优先，给一个TaskSet分配资源，找不到（lanunchedTask为ifalse才会到下一个级别）
    // 3. （封装在resourceOfferSingleTaskSet函数）在多次遍历offer list(tasks)
    // 因为一次taskSet.resourceOffer只会占用一个core
    // 而不是一次用光所有的core， 这样有助于一个taskset中的task比较均匀的分步在workers上
    // 4. 只要在该taskSet，改locality下，对所有worker offer都找不到合适task时才跳到下个locality级别
    var launchedTask = false
    for (taskSet <- sortedTaskSets; maxLocality <- taskSet.myLocalityLevels) {
      do {
        launchedTask = resourceOfferSingleTaskSet(
            taskSet, maxLocality, shuffledOffers, availableCpus, tasks)
      } while (launchedTask)
    }

    if (tasks.size > 0) {
      hasLaunchedTask = true
    }
    return tasks
  }

  ```

相关类定义：
WorkerOffer：Represents free resources available on an executor.
TaskDescription: Description of a task that gets passed onto executors to be executed, usually created by [[TaskSetManager.resourceOffer]].

---

## Task运行期的函数调用
  * TaskRunner.run
    * Task.run
      * Task.runTask(Task是一个基类，有两个子类，分别为ShuffleTask和ResultTask)
        * RDD.iterator
          * RDD.computeOrReadCheckpoint
            * RDD.compute
或许当看到RDD.compute函数定义时，还是觉着方法没有被调用，MappedRDD的compute为例 

```
override def compute(split: Partition, context: TaskContext) = 

firstParent[T]。iterator(split, context).map(f)
```
注意，这里最容易产生错误的地方就是map函数，这里的map不是RDD中的map，`而是Scala中定义的iterator的成员函数map`（已经具体到数据级别）
请自行参考http://www.scala-lang.org/api/2.10.4/index.html#scala.collection.Iterator

